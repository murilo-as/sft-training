model:
  id: "Qwen/Qwen3-30B-A3B-Instruct-2507"
  device_map: "auto" #dispositivo para carregar o modelo
data:
  train_path: "data/perguntas_train.jsonl"
  processed_path: "data/train_processed.jsonl"
output:
  dir: "outputs/qwen3-30b-sft"

quantization:
  load_in_4bit: true #ativa a quantização de 4 bits
  bnb_4bit_use_double_quant: true #quantizacao do fator de escala
  bnb_4bit_quant_type: "nf4" #fp4 (distribuicao assimetrica) ou nf4 (distribuição normal)
  bnb_4bit_compute_dtype: "bfloat16" # float ou bfloat16

lora: 
  r: 16 #dimensao do subespaço
  lora_alpha: 32 #escala de atualização
  lora_dropout: 0.05 
  bias: "none" 
  task_type: "CAUSAL_LM" 
  target_modules: ["q_proj", "v_proj"]

training:
  per_device_train_batch_size: 1 #tamanho do batch por dispositivo 
  gradient_accumulation_steps: 4 #numero de passos até atualizar os pesos
  num_train_epochs: 4 #numero de épocas
  learning_rate: 0.0002 #taxa de aprendizado
  optimizer: "adamw_torch" #otimizador
  fp16: false
  bf16: true
  logging_steps: 1 
  save_steps: 20 #salva o modelo a cada 20 passos
  save_total_limit: 10 #limite de checkpoints salvos
  report_to: "wandb"
  run_name: "qwen3-30b-sft"
  